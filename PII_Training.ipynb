{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "vFjCjapviAN6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U2PJvQgqh5yK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pandas as pd \n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.optimization import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Functions"
      ],
      "metadata": {
        "id": "2TAcJ3WxiGoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_most_probable(start_scores: torch.Tensor, end_scores: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Given a batch of start and end of span scores (logits), returns the\n",
        "    most probable (start, end) indexes for each sample in the batch\n",
        "    such that end >= start.\n",
        "\n",
        "    Args:\n",
        "        start_scores (torch.Tensor): start position scores of shape (B, T)\n",
        "            of for each token position.\n",
        "        end_scores (torch.Tensor): end position scores of shape (B, T)\n",
        "            of for each token position.\n",
        "    Returns:\n",
        "        start_end_indexes (Tuple[torch.Tensor, torch.Tensor]): tuple of \n",
        "            tensors containing start and end indexes respectively for each\n",
        "            sample.\n",
        "    \"\"\"\n",
        "\n",
        "    # extract shapes\n",
        "    batch_dim, timestep_dim = start_scores.shape\n",
        "    # compute marginal distributions for start and end\n",
        "    start_probs = torch.nn.functional.softmax(start_scores, dim=1)\n",
        "    end_probs = torch.nn.functional.softmax(end_scores, dim=1)\n",
        "    # compute start_end joint distribution\n",
        "    joint_dist_start_end = start_probs[:, :, None] @ end_probs[:, None, :]\n",
        "    constrained_joint_dist = torch.triu(joint_dist_start_end)\n",
        "    # compute the actual indexes\n",
        "    flattened_distr_argmax = constrained_joint_dist.view(batch_dim, -1).argmax(1).view(-1, 1)\n",
        "    start_end_idxs = torch.cat((flattened_distr_argmax // timestep_dim, flattened_distr_argmax % timestep_dim), dim=1).cpu().detach()\n",
        "    return (start_end_idxs[:, 0], start_end_idxs[:, 1])"
      ],
      "metadata": {
        "id": "d_GvNqBHiKJ_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrame(data,ratio=0.7,seed=42,limit=None):\n",
        "    ar = data.conversation_id.unique()\n",
        "    if limit:\n",
        "        ar = ar[:limit]\n",
        "    random.shuffle(ar)\n",
        "    train_count = int(ar.shape[0]* 0.7)\n",
        "    train_ar = ar[:train_count]\n",
        "    val_ar = ar[train_count:]\n",
        "    train_df = data[data.conversation_id.isin(train_ar)].reset_index(drop=True)\n",
        "    val_df = data[data.conversation_id.isin(val_ar)].reset_index(drop=True)\n",
        "    return train_df,val_df"
      ],
      "metadata": {
        "id": "cG71Y-0-iLc_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_padder_collate_fn(sample_list):\n",
        "    # NOTE: the tokenizer in dataloader already pads inputs to have same length of 384\n",
        "    input_ids_padded = [sample[\"input_ids\"] for sample in sample_list]\n",
        "    attention_mask_padded = [sample[\"attention_mask\"] for sample in sample_list]\n",
        "    out = [sample[\"out_span\"] for sample in sample_list]\n",
        "    # Convert inputs to Torch tensors\n",
        "    input_ids_padded = torch.tensor(input_ids_padded, dtype=torch.long)\n",
        "    attention_mask_padded = torch.tensor(attention_mask_padded, dtype=torch.long)\n",
        "    # Tensor adds an extra dimension, so remove it\n",
        "    input_ids_padded = input_ids_padded[:, 0, :]\n",
        "    attention_mask_padded = attention_mask_padded[:, 0, :]\n",
        "    return {\"input_ids\": input_ids_padded,\n",
        "            \"attention_mask\": attention_mask_padded,\n",
        "            \"y_gt\":torch.stack(out)}"
      ],
      "metadata": {
        "id": "GQ6B7N_1iNDG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_tokenizer_fn(question, paragraph, tokenizer, max_length=384, doc_stride=128):\n",
        "    pad_on_right = tokenizer.padding_side == \"right\"\n",
        "    # Process the sample\n",
        "    tokenized_input_pair = tokenizer(\n",
        "        question,\n",
        "        paragraph,\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return tokenized_input_pair"
      ],
      "metadata": {
        "id": "WyQTGqW1iOoP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Classes"
      ],
      "metadata": {
        "id": "bE5DbjxqiTJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParametricBertModelQA(torch.nn.Module):\n",
        "    def __init__(self,model_name):\n",
        "        super(ParametricBertModelQA,self).__init__()\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    \n",
        "    def prepare_input_fn(self,inputs, device):\n",
        "        model_input = {}\n",
        "        model_input[\"input_ids\"] = inputs[\"input_ids\"].to(device)\n",
        "        model_input[\"attention_mask\"] = inputs[\"attention_mask\"].to(device)\n",
        "        return model_input\n",
        "    \n",
        "    def forward(self,inputs):\n",
        "        curr_device = self.model.device\n",
        "        input_dict = self.prepare_input_fn(inputs,curr_device)\n",
        "        output = self.model(**input_dict)\n",
        "        return (output.start_logits,output.end_logits)"
      ],
      "metadata": {
        "id": "F4U7a0waiVUS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomQADatasetBERT(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom text dataset for Huggingface BERT models.\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer_fn, df):\n",
        "        super(CustomQADatasetBERT, self).__init__()\n",
        "        self.input_list = df[[\"paragraph\", \"question_text\", \"question_id\"]]\n",
        "        self.output_list = df[[\"tokenizer_answer_start\", \"tokenizer_answer_end\"]]\n",
        "        self.tokenizer_fn = tokenizer_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        paragraph_text = self.input_list.iloc[idx][\"paragraph\"]\n",
        "        question_id = self.input_list.iloc[idx][\"question_id\"]\n",
        "        question_text = self.input_list.iloc[idx][\"question_text\"]\n",
        "        tokenizer_answer_start = self.output_list.iloc[idx][\"tokenizer_answer_start\"]\n",
        "        tokenizer_answer_end = self.output_list.iloc[idx][\"tokenizer_answer_end\"]\n",
        "\n",
        "        tokenized_input_pair = self.tokenizer_fn(question_text, paragraph_text)\n",
        "        \n",
        "        input_ids = tokenized_input_pair[\"input_ids\"]\n",
        "        attention_mask = tokenized_input_pair[\"attention_mask\"]\n",
        "\n",
        "        out_span = torch.tensor([tokenizer_answer_start, tokenizer_answer_end])\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"out_span\": out_span,\n",
        "            \"question\":question_text\n",
        "        }"
      ],
      "metadata": {
        "id": "GmKfiWsciYV5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelUtils:\n",
        "    def __init__(self,model,create=False):\n",
        "        cur_drive = 'drive/MyDrive/'\n",
        "        if not create:\n",
        "            self.model_path = os.path.join(cur_drive,model)\n",
        "        else:\n",
        "            self.dir_path = os.path.join(cur_drive,model)\n",
        "            self.model_path = self._create_dir()\n",
        "         \n",
        "    def save_params(self,params_dict):\n",
        "        with open(os.path.join(self.model_path,'params_dict.pkl'),'wb') as fp:\n",
        "            pickle.dump(params_dict, fp)\n",
        "    \n",
        "    def _create_dir(self):\n",
        "        #check if dir exist and create if not\n",
        "        addPath = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
        "        if not os.path.exists(self.dir_path):\n",
        "            os.makedirs(self.dir_path)\n",
        "        model_path = os.path.join(self.dir_path,addPath)\n",
        "        os.makedirs(model_path)\n",
        "        return model_path\n",
        "    \n",
        "    def _saveResult(self,resJson):\n",
        "        with open(os.path.join(self.model_path,'training_stats.json'),'w') as fp:\n",
        "            import json\n",
        "            fp.write(json.dumps(resJson))\n",
        "\n",
        "    def save_checkpoints(self,state,isBest=False):\n",
        "        \"\"\"\n",
        "        This func saves below details -\n",
        "        * trained model \n",
        "        * hyperparameter\n",
        "        \"\"\"\n",
        "        filename = 'best_ckps.pt' if isBest else 'last_ckps.pt'\n",
        "        torch.save(state,os.path.join(self.model_path,filename))\n",
        "        \n",
        "    def save_best_model(self,model):\n",
        "        checkpoint = torch.load(f'{self.model_path}/best_ckps.pt')\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        model.model.save_pretrained(f'{self.model_path}/')\n",
        "        print(f'----- model is saved at {self.model_path} ------')"
      ],
      "metadata": {
        "id": "ipPqG3MniaL5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAModel:\n",
        "    def __init__(self,train_args=None,load_args=None):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if load_args:\n",
        "            self.utilsObj = ModelUtils(load_args['model_ckp'])\n",
        "            with open(os.path.join(self.utilsObj.model_path,'params_dict.pkl'),'rb') as fp:\n",
        "                self.params_dict = pickle.load(fp)\n",
        "            self.model,self.optimizer = self.loadModel(load_model=True)\n",
        "            self.tokenizer_dict = self.loadTokenizer(load_model=True)\n",
        "            if load_args['ckp_status']:\n",
        "                ckp_path = os.path.join(self.utilsObj.model_path,f'{load_args[\"ckp_status\"]}_ckps.pt')\n",
        "                checkpoint = torch.load(ckp_path)\n",
        "                self.model.load_state_dict(checkpoint['state_dict'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                self.epoch_start = checkpoint['epoch']\n",
        "                self.val_min_loss = checkpoint['validation_loss']\n",
        "        else:\n",
        "            self.params_dict = train_args['params_dict']\n",
        "            self.utilsObj = ModelUtils(train_args['folder_name'],create=True)\n",
        "            self.tokenizer_dict = self.loadTokenizer()\n",
        "            self.model,self.optimizer = self.loadModel()\n",
        "            self.val_min_loss = np.Inf\n",
        "            self.epoch_start = -1\n",
        "            \n",
        "    def index_of_first(self,lst, pred):\n",
        "        for i, v in enumerate(lst):\n",
        "            if pred(v):\n",
        "                return i\n",
        "        return None\n",
        "\n",
        "    def split_paragraph_if_needed(self,paragraph, question, answer_span):\n",
        "        \"\"\"\n",
        "        Attempts to tokenize a paragraph and question together, if too long\n",
        "        because of tokenizer's max length, then will split the paragraph into\n",
        "        multiple slices.\n",
        "\n",
        "        Returns a list of paragraph slices with answer span, such that:\n",
        "            - a paragraph slice with no answer will have answer mapped to (CLS, CLS)\n",
        "            - a paragraph slice with answer will be mapped to the index of answer.\n",
        "        \"\"\"\n",
        "        tokenized_input_pair = self.tokenizer_dict['tokenizer_process'](question, paragraph)\n",
        "        # outputs\n",
        "        paragraph_splits = []\n",
        "        answer_spans = []\n",
        "        # get answer end char idx\n",
        "        ans_start = answer_span[0]\n",
        "        ans_end = answer_span[1]\n",
        "        \"\"\"\n",
        "        1) Find index of context segments in the tokenized example\n",
        "        2) Within the context segments (start from context_segment_idx), \n",
        "           find the token corresponding to span of answer: start and end.\n",
        "        \"\"\"\n",
        "        for offset_idx, offset in enumerate(tokenized_input_pair.offset_mapping):\n",
        "            # get sequence ids\n",
        "            sequence_ids = tokenized_input_pair.sequence_ids(offset_idx)\n",
        "            # find start index of context segment\n",
        "            context_segment_idx = sequence_ids.index(1)\n",
        "            span_start_offset_idx = self.index_of_first(\n",
        "                tokenized_input_pair.offset_mapping[offset_idx][context_segment_idx:], \n",
        "                lambda span: span[0] <= ans_start <= span[1]\n",
        "            )\n",
        "            span_end_offset_idx = self.index_of_first(\n",
        "                tokenized_input_pair.offset_mapping[offset_idx][context_segment_idx:], \n",
        "                lambda span: span[0] <= ans_end <= span[1]\n",
        "            )\n",
        "            # Decode split into a string\n",
        "            decoded_split = self.tokenizer_dict['tokenizer'].decode(tokenized_input_pair.input_ids[offset_idx][context_segment_idx:], skip_special_tokens=True)\n",
        "            # \n",
        "            paragraph_splits.append(decoded_split)\n",
        "            if span_start_offset_idx is not None and span_end_offset_idx is not None:\n",
        "                # If answer span is fully in current slice\n",
        "                # add segment idx offset\n",
        "                span_start_offset_idx += context_segment_idx\n",
        "                span_end_offset_idx += context_segment_idx + 1 # the plus 1 is needed for correct slicing\n",
        "                answer_spans.append((span_start_offset_idx, span_end_offset_idx))\n",
        "            elif span_start_offset_idx is None and span_end_offset_idx is None:\n",
        "                # If span not in this slice, but in another slice\n",
        "                # map answer to (CLS, CLS)\n",
        "                cls_idx = tokenized_input_pair.input_ids[offset_idx].index(self.tokenizer_dict['tokenizer'].cls_token_id)\n",
        "                # NOTE(Alex): although I think it's always 0\n",
        "                answer_spans.append((cls_idx, cls_idx))\n",
        "            else:\n",
        "                # span spans along multiple slices -> throw the sample away \n",
        "                # (should be only like 4 samples across the whole dataset)\n",
        "                # Discard sample\n",
        "                pass\n",
        "\n",
        "        return (paragraph_splits, answer_spans)   \n",
        "\n",
        "    def preprocessData(self,data):\n",
        "        dataframe_list = []\n",
        "        for conv_id in tqdm(data.conversation_id.unique()):\n",
        "#            try:\n",
        "            conversation = data[data.conversation_id == conv_id].context.unique()[0]\n",
        "            question = data[data.conversation_id == conv_id].question.tolist()\n",
        "            answer = data[data.conversation_id == conv_id].answers.tolist()\n",
        "            ## for one conversation_id - \n",
        "            for idx in range(len(question)):\n",
        "                ##for each question inside conversation\n",
        "                ques_text = question[idx].strip()\n",
        "                answer_text = eval(answer[idx])['text'].strip()\n",
        "                answer_start = eval(answer[idx])['answer_start']\n",
        "                answer_end = answer_start + len(answer_text)\n",
        "                par_splits, split_answer_spans = self.split_paragraph_if_needed(\n",
        "                                    conversation, \n",
        "                                    ques_text, \n",
        "                                    (answer_start, answer_end)\n",
        "                                )\n",
        "                #pair_overflows = len(par_splits) > 1\n",
        "                for split_idx, (split_text, split_ans_span) in enumerate(zip(par_splits, split_answer_spans)):\n",
        "                    #print(answer_text)\n",
        "                    dataframe_list.append({\n",
        "                        'conversation_id':conv_id,\n",
        "                        'paragraph':split_text,\n",
        "                        'question_id':idx,\n",
        "                        'question_text':ques_text,\n",
        "                        'answer_text':answer_text,\n",
        "                        'answer_start':answer_start,\n",
        "                        'tokenizer_answer_start':split_ans_span[0],\n",
        "                        'tokenizer_answer_end':split_ans_span[1],\n",
        "                    })\n",
        "                #print(f'proccesed conv_id - {conv_id}')\n",
        "#             except:\n",
        "#                 print(f'conversation_id - {conv_id}')\n",
        "        return pd.DataFrame(dataframe_list)\n",
        "    \n",
        "    def get_params_for_optimizer(self,model, no_decay, weight_decay=0.0001):\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                'params': [\n",
        "                    p for n, p in param_optimizer if not any(\n",
        "                        nd in n for nd in no_decay\n",
        "                    )\n",
        "                ], \n",
        "                'weight_decay': weight_decay\n",
        "            },\n",
        "            {\n",
        "                'params': [\n",
        "                    p for n, p in param_optimizer if any(\n",
        "                        nd in n for nd in no_decay\n",
        "                    )\n",
        "                ],\n",
        "                'weight_decay': 0.0\n",
        "            },\n",
        "        ]\n",
        "        return optimizer_parameters\n",
        "    \n",
        "    \n",
        "    def loadModel(self,load_model=False):\n",
        "        if load_model:\n",
        "            model = ParametricBertModelQA(self.utilsObj.model_path)\n",
        "        else:\n",
        "            model = ParametricBertModelQA(self.params_dict['model_url'])\n",
        "            model.model.save_pretrained(f'{self.utilsObj.model_path}/')\n",
        "        \n",
        "        if self.device=='cuda':\n",
        "            model = model.cuda()\n",
        "        \n",
        "        # Define parameters on which to apply L2 decay\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        if self.params_dict[\"train_params\"][\"weight_decay\"] > 0.0:\n",
        "            model_params_optimizer = self.get_params_for_optimizer(model, no_decay, weight_decay=self.params_dict[\"train_params\"][\"weight_decay\"])\n",
        "        else:\n",
        "            model_params_optimizer = model.parameters()\n",
        "\n",
        "        # Define optimizer\n",
        "        optimizer = AdamW(\n",
        "            model_params_optimizer, \n",
        "            lr=params_dict[\"train_params\"][\"initial_lr\"], \n",
        "            correct_bias=False\n",
        "        )\n",
        "        return model,optimizer\n",
        "        \n",
        "    def loadTokenizer(self,load_model=False):\n",
        "        if load_model:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.utilsObj.model_path)\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.params_dict[\"tokenizer_url\"])\n",
        "        tokenizer_fn_preprocess = partial(bert_tokenizer_fn, tokenizer=tokenizer, max_length=self.params_dict[\"tokenizer_max_length\"]-3)\n",
        "        tokenizer_fn_train = partial(bert_tokenizer_fn, tokenizer=tokenizer, max_length=self.params_dict[\"tokenizer_max_length\"])\n",
        "        tokenizer.save_pretrained(f'{self.utilsObj.model_path}/')\n",
        "        tokenizer_dict = {\"tokenizer\":tokenizer,\n",
        "                          \"tokenizer_process\":tokenizer_fn_preprocess,\n",
        "                          \"tokenizer_train\":tokenizer_fn_train}\n",
        "        return tokenizer_dict\n",
        "    \n",
        "    def train_step(self,scaler, loss_function, dataloader, scheduler=None, device=\"cpu\", show_progress=False):\n",
        "        acc_loss = 0\n",
        "        acc_start_accuracy = 0\n",
        "        acc_end_accuracy = 0\n",
        "        count = 0\n",
        "\n",
        "        time_start = timer()\n",
        "\n",
        "        self.model.train()\n",
        "        wrapped_dataloader = tqdm(dataloader) if show_progress else dataloader\n",
        "        for batch in wrapped_dataloader:\n",
        "            # NOTE: we'll pass directly the batch dict to the model for inputs.\n",
        "            answer_spans_start = batch[\"y_gt\"][:, 0]\n",
        "            answer_spans_end = batch[\"y_gt\"][:, 1]\n",
        "            # Clear gradients\n",
        "            self.model.zero_grad()\n",
        "            # Place to right device\n",
        "            answer_spans_start = answer_spans_start.to(device)\n",
        "            answer_spans_end = answer_spans_end.to(device)\n",
        "            # Use Automatic Mixed Precision if enabled\n",
        "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "                # Run forward pass\n",
        "                pred_answer_start_scores, pred_answer_end_scores = self.model(batch)\n",
        "                pred_answer_start_scores = pred_answer_start_scores.to(device)\n",
        "                pred_answer_end_scores = pred_answer_end_scores.to(device)\n",
        "\n",
        "                # Compute the CrossEntropyLoss\n",
        "                loss = (loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end))/2.0\n",
        "            scaler.scale(loss).backward()\n",
        "            # Optimizer step (via scaler)\n",
        "            scaler.step(self.optimizer)\n",
        "            scaler.update()\n",
        "            # Update LR scheduler\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            # --- Compute metrics ---\n",
        "            # Get span indexes\n",
        "            pred_span_start_idxs, pred_span_end_idxs = extract_most_probable(pred_answer_start_scores, pred_answer_end_scores)\n",
        "            gt_start_idxs = answer_spans_start.cpu().detach()\n",
        "            gt_end_idxs = answer_spans_end.cpu().detach()\n",
        "            # two accs\n",
        "            start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
        "            end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
        "            # Gather stats\n",
        "            acc_loss += loss.item()\n",
        "            acc_start_accuracy += start_accuracy.item()\n",
        "            acc_end_accuracy += end_accuracy.item()\n",
        "            count += 1\n",
        "        time_end = timer()\n",
        "        return {\n",
        "            \"loss\": acc_loss / count, \n",
        "            \"accuracy_start\": acc_start_accuracy / count, \n",
        "            \"accuracy_end\": acc_end_accuracy / count,\n",
        "            \"time\": time_end - time_start\n",
        "        }\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def validation_step(self,scaler, loss_function, dataloader, device=\"cpu\", show_progress=False):\n",
        "        acc_loss = 0\n",
        "        acc_start_accuracy = 0\n",
        "        acc_end_accuracy = 0\n",
        "        count = 0\n",
        "\n",
        "        time_start = timer()\n",
        "        wrapped_dataloader = tqdm(dataloader) if show_progress else dataloader\n",
        "\n",
        "        self.model.eval()\n",
        "        for batch in wrapped_dataloader:\n",
        "            answer_spans_start = batch[\"y_gt\"][:, 0]\n",
        "            answer_spans_end = batch[\"y_gt\"][:, 1]\n",
        "            # Place to right device\n",
        "            answer_spans_start = answer_spans_start.to(device)\n",
        "            answer_spans_end = answer_spans_end.to(device)\n",
        "            # Use Automatic Mixed Precision if enabled\n",
        "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "                # Run forward pass\n",
        "                pred_answer_start_scores, pred_answer_end_scores = self.model(batch)\n",
        "                pred_answer_start_scores = pred_answer_start_scores.to(device)\n",
        "                pred_answer_end_scores = pred_answer_end_scores.to(device)\n",
        "                # Compute the CrossEntropyLoss\n",
        "                loss = (loss_function(pred_answer_start_scores, answer_spans_start) + loss_function(pred_answer_end_scores, answer_spans_end))/2.0\n",
        "            # --- Compute metrics ---\n",
        "            # Get span indexes\n",
        "            pred_span_start_idxs, pred_span_end_idxs = extract_most_probable(pred_answer_start_scores, pred_answer_end_scores)\n",
        "            gt_start_idxs = answer_spans_start.cpu().detach()\n",
        "            gt_end_idxs = answer_spans_end.cpu().detach()\n",
        "            # two accs\n",
        "            start_accuracy = torch.sum(gt_start_idxs == pred_span_start_idxs) / len(pred_span_start_idxs)\n",
        "            end_accuracy = torch.sum(gt_end_idxs == pred_span_end_idxs) / len(pred_span_end_idxs)\n",
        "            # Gather stats\n",
        "            acc_loss += loss.item()\n",
        "            acc_start_accuracy += start_accuracy.item()\n",
        "            acc_end_accuracy += end_accuracy.item()\n",
        "            count += 1\n",
        "        time_end = timer()\n",
        "        return {\n",
        "            \"loss\": acc_loss / count, \n",
        "            \"accuracy_start\": acc_start_accuracy / count, \n",
        "            \"accuracy_end\": acc_end_accuracy / count,\n",
        "            \"time\": time_end - time_start\n",
        "        }        \n",
        "    \n",
        "    def train_model(self,train_process_df,val_process_df,epochs=None,batch_size=None,use_amp = True):\n",
        "\n",
        "        if epochs:\n",
        "            self.params_dict[\"train_params\"][\"epochs\"] = epochs\n",
        "        if batch_size:\n",
        "            self.params_dict['train_params']['batch_size_train'] = batch_size\n",
        "            self.params_dict['train_params']['batch_size_val'] = batch_size\n",
        "        # Estimate the number of train steps for LR scheduler\n",
        "        num_train_steps = int(\n",
        "            (len(train_process_df) / self.params_dict[\"train_params\"][\"batch_size_train\"]) * self.params_dict[\"train_params\"][\"epochs\"]\n",
        "        )\n",
        "\n",
        "        num_warmup_steps = int(num_train_steps * 0.1) # 10% of warmup steps\n",
        "\n",
        "        # LR scheduler\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_train_steps\n",
        "        )\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        if self.device=='cuda':\n",
        "            loss_function = loss_function.cuda()\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        history = {\n",
        "            \"train_loss\": [], \"train_acc_start\": [], \"train_acc_end\": [],\n",
        "            \"val_loss\": [], \"val_acc_start\": [], \"val_acc_end\": []\n",
        "        }\n",
        "        #creating dataloader\n",
        "        train_dataset = CustomQADatasetBERT(self.tokenizer_dict['tokenizer_train'],train_process_df)\n",
        "        val_dataset = CustomQADatasetBERT(self.tokenizer_dict['tokenizer_train'],val_process_df)\n",
        "        \n",
        "        train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                                collate_fn = bert_padder_collate_fn, \n",
        "                                                batch_size=self.params_dict['train_params']['batch_size_train'],\n",
        "                                                shuffle=True)\n",
        "        val_data_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                              collate_fn = bert_padder_collate_fn, \n",
        "                                              batch_size=self.params_dict['train_params']['batch_size_val'], \n",
        "                                              shuffle=True)\n",
        "        \n",
        "        loop_start = timer()\n",
        "        self.utilsObj.save_params(self.params_dict)\n",
        "        for epoch in range(self.epoch_start+1,self.params_dict[\"train_params\"][\"epochs\"]):\n",
        "            train_dict = self.train_step(scaler, loss_function, train_data_loader,scheduler=scheduler,device=self.device,show_progress=True)\n",
        "            val_dict = self.validation_step(scaler, loss_function, val_data_loader,device=self.device)\n",
        "            cur_lr = self.optimizer.param_groups[0]['lr']\n",
        "            checkpoints = {\n",
        "              'epoch':epoch,\n",
        "              'state_dict':self.model.state_dict(),\n",
        "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "              'validation_loss':val_dict['loss'],\n",
        "              'training_loss':train_dict['loss']\n",
        "            }\n",
        "            self.utilsObj.save_checkpoints(checkpoints)\n",
        "            if val_dict[\"loss\"] < self.val_min_loss:\n",
        "                print(f'validation loss decreased {self.val_min_loss:.3f} --> {val_dict[\"loss\"]:.3f}')\n",
        "                self.val_min_loss = val_dict[\"loss\"]\n",
        "                self.utilsObj.save_checkpoints(checkpoints,True)\n",
        "            print(f'Epoch: {epoch}, '\n",
        "                  f'lr: {cur_lr}, '\n",
        "                  f'Train loss: {train_dict[\"loss\"]:.4f}, '\n",
        "                  f'Train acc start: {train_dict[\"accuracy_start\"]:.4f}, '\n",
        "                  f'Train acc end: {train_dict[\"accuracy_end\"]:.4f}, '\n",
        "                  f'Val loss: {val_dict[\"loss\"]:.4f}, '\n",
        "                  f'Val acc start: {val_dict[\"accuracy_start\"]:.4f}, '\n",
        "                  f'Val acc end: {val_dict[\"accuracy_end\"]:.4f}, '\n",
        "                  f'Time: {train_dict[\"time\"]:.4f}')\n",
        "            history[\"train_loss\"].append(train_dict[\"loss\"]);history[\"train_acc_start\"].append(train_dict[\"accuracy_start\"]);history[\"train_acc_end\"].append(train_dict[\"accuracy_end\"]);\n",
        "            history[\"val_loss\"].append(val_dict[\"loss\"]);history[\"val_acc_start\"].append(val_dict[\"accuracy_start\"]);history[\"val_acc_end\"].append(val_dict[\"accuracy_end\"]);\n",
        "        loop_end = timer()\n",
        "        print(f\"Elapsed time: {(loop_end - loop_start):.4f}\")\n",
        "        self.utilsObj._saveResult(history)\n",
        "        self.utilsObj.save_best_model(self.model)"
      ],
      "metadata": {
        "id": "oCI_11i0idMH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Function"
      ],
      "metadata": {
        "id": "vCt2IKFnie1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('drive/MyDrive/PII_Training/squad.xlsx')"
      ],
      "metadata": {
        "id": "0e5Pqt5Xih-v"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-zQzP9Khkg-k",
        "outputId": "877cc99a-af8a-4369-a2ce-44a0d25f43ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conversation_id                                            context  \\\n",
              "0             2571  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "1             2571  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "2             2571  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "3             2571  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "4             2571  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "\n",
              "                                            question  \\\n",
              "0           When did Beyonce start becoming popular?   \n",
              "1  What areas did Beyonce compete in when she was...   \n",
              "2  When did Beyonce leave Destiny's Child and bec...   \n",
              "3      In what city and state did Beyonce  grow up?    \n",
              "4         In which decade did Beyonce become famous?   \n",
              "\n",
              "                                             answers  \n",
              "0  {'text': 'in the late 1990s', 'answer_start': ...  \n",
              "1  {'text': 'singing and dancing', 'answer_start'...  \n",
              "2              {'text': '2003', 'answer_start': 526}  \n",
              "3    {'text': 'Houston, Texas', 'answer_start': 166}  \n",
              "4        {'text': 'late 1990s', 'answer_start': 276}  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d22161b6-0f90-4e93-ae05-fc8faef1dab1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2571</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>{'text': 'in the late 1990s', 'answer_start': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2571</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>{'text': 'singing and dancing', 'answer_start'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2571</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>{'text': '2003', 'answer_start': 526}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2571</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>{'text': 'Houston, Texas', 'answer_start': 166}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2571</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>{'text': 'late 1990s', 'answer_start': 276}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d22161b6-0f90-4e93-ae05-fc8faef1dab1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d22161b6-0f90-4e93-ae05-fc8faef1dab1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d22161b6-0f90-4e93-ae05-fc8faef1dab1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,val_df = splitDataFrame(data,limit=20)"
      ],
      "metadata": {
        "id": "BPMzKqRLk0w9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape,val_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNoJDAZ0k21c",
        "outputId": "5ab73aef-fe6b-4150-b1be-3dcf07253d95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((175, 4), (76, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_dict = {\n",
        "        \"model_url\": \"distilbert-base-cased-distilled-squad\",\n",
        "        \"tokenizer_url\": \"distilbert-base-cased-distilled-squad\",\n",
        "        \"tokenizer_max_length\": 384,\n",
        "        \"train_params\": {\n",
        "            \"epochs\": 2,\n",
        "            \"initial_lr\": 0.00003,\n",
        "            \"batch_size_train\": 8,\n",
        "            \"batch_size_val\": 8,\n",
        "            \"weight_decay\": 0.01\n",
        "        } }"
      ],
      "metadata": {
        "id": "DGHfjvTYk4Yd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "kFdwJIHpk72o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = {'params_dict':params_dict,\n",
        "       'folder_name':'distilbert'}"
      ],
      "metadata": {
        "id": "ILp15jG4k6fY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelObj = QAModel(train_args=train_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXaskpTAk-1r",
        "outputId": "54cc9f1e-6584-4316-fb31-a4a14b65564f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_process_df = modelObj.preprocessData(train_df)\n",
        "val_process_df = modelObj.preprocessData(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q82-VyZglAJ1",
        "outputId": "bcfe3e26-04d4-42f8-8b97-820644ed9fda"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 20.88it/s]\n",
            "100%|██████████| 6/6 [00:00<00:00, 13.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelObj.train_model(train_process_df,val_process_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i9udwj0lDCe",
        "outputId": "8216b20e-643d-4193-882c-e9ceafef7c9c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/24 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "100%|██████████| 24/24 [00:06<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss decreased inf --> 1.711\n",
            "Epoch: 0, lr: 1.6046511627906977e-05, Train loss: 2.3701, Train acc start: 0.6438, Train acc end: 0.3000, Val loss: 1.7114, Val acc start: 0.7250, Val acc end: 0.4500, Time: 6.5086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:03<00:00,  7.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss decreased 1.711 --> 1.446\n",
            "Epoch: 1, lr: 0.0, Train loss: 1.2061, Train acc start: 0.7156, Train acc end: 0.5719, Val loss: 1.4459, Val acc start: 0.7125, Val acc end: 0.5000, Time: 3.0527\n",
            "Elapsed time: 23.7825\n",
            "----- model is saved at drive/MyDrive/distilbert/2023_02_22_054405 ------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load checkpoints"
      ],
      "metadata": {
        "id": "Gs0qvqvmq3b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_args = {'model_ckp':'distilbert/2023_02_22_054405',\n",
        "'ckp_status':'last'}"
      ],
      "metadata": {
        "id": "VdLDhMpFlE5-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelObjckp = QAModel(load_args=load_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNdY6eqXq7QH",
        "outputId": "87ec769c-de5e-4691-fd99-963ab8db8053"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelObjckp.train_model(train_process_df,val_process_df,epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLD0JvU5sEth",
        "outputId": "00c68e0d-6159-432f-e56a-4193080d55ca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:03<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss decreased 1.446 --> 1.404\n",
            "Epoch: 2, lr: 2.635514018691589e-05, Train loss: 0.6235, Train acc start: 0.8042, Train acc end: 0.7604, Val loss: 1.4040, Val acc start: 0.7375, Val acc end: 0.5375, Time: 3.0719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:03<00:00,  7.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, lr: 1.9626168224299065e-05, Train loss: 0.3828, Train acc start: 0.9010, Train acc end: 0.8958, Val loss: 1.5869, Val acc start: 0.7625, Val acc end: 0.6125, Time: 3.1392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:03<00:00,  7.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, lr: 1.2897196261682243e-05, Train loss: 0.2630, Train acc start: 0.9375, Train acc end: 0.9115, Val loss: 1.6107, Val acc start: 0.7250, Val acc end: 0.5875, Time: 3.1240\n",
            "Elapsed time: 25.4227\n",
            "----- model is saved at drive/MyDrive/distilbert/2023_02_22_054405 ------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Predict"
      ],
      "metadata": {
        "id": "gedK--4j1P-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "-EO_jdHSscXr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = data.context.iloc[0]\n",
        "ques = data.question.iloc[0]\n",
        "answer = data.answers.iloc[0]"
      ],
      "metadata": {
        "id": "leVP8zNa1R_a"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = pipeline(\"question-answering\", model='drive/MyDrive/distilbert/2023_02_22_054405/')"
      ],
      "metadata": {
        "id": "212SYZn11S4j"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model(question=ques,context=context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiYZ9_a_1XwU",
        "outputId": "0df7012e-3c75-499f-9441-fe92c67b776c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.6709728837013245,\n",
              " 'start': 281,\n",
              " 'end': 294,\n",
              " 'answer': '1990s as lead'}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jslVqoF1hJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}